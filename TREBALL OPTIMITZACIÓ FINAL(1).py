#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')


# In[2]:


data = pd.read_csv('bankruptcy.csv')


# # DATASET
# 
# 
# Once the data was imported, it was decided to creat two new ratios constructed by the combination of other two ratios in the dataset.
#  
#  * ROE: 
#  
# Return on equity (ROE) measures a corporation's profitability in relation to stockholdersâ€™ equity (EBIT / EQ). This variable was not in the full data frame but it could be constructed and after added to ou dataset. It is the combination of Attr 7 / Attr 10 = (EBIT / TA) / (EQ / TA) = (EBIT / EQ).
#    
#      
#  * Quality of the debt: 
#  
# This ratio captures how is the debt structured. A high ratio would imply that the firm has a great share of current liabilities over total liabilites. (CL / TL). Once again, it was not on the data frame but could be generated by the combination of: Attr50 / Attr4 = (CA / CL) / (CA / TL) = CL / TL.
# 
# 
# Now, it was considered considered that not all the indicators given would be an explanatory features, so it was needed to filter those ratios that were crucial in order to predict the bankruptcy of firms. To clarify the used data set the name of the selected variable was changed as follows.
# 
#  
#  # Variables:
#  
# * TL / TA : total liabilities / total assets 
# * CA / CL : current assets / short-term liabilities  
# * EQ / (TL + EQ): equity / total asset ; Remember that TA = TL + EQ
# * Quality_debt : CL / TL
# * Net Profit / Sales :  net profit / total assets 
# * ROE : EBIT / EQ
# * ROA : EBIT / total assets 
# * EQ / (TL + EQ): equity / total asset ; Remember that TA = TL + EQ

# In[3]:


data['ROE'] = data['Attr7']/data['Attr10']
data['quality_debt'] = data['Attr50']/data['Attr4']

d = data.rename({
    'class': 'bankruptcy',
    'Attr2': 'TL / TA',
    'Attr3': 'Working K / TA',
    'Attr4': 'CA / CL',
    'Attr7': 'ROA',
    'Attr23': 'Net Profit / Sales',
    'Attr10': 'EQ / (TL + EQ)',
}, axis=1)

d = d.filter(['TL / TA', 'Working K / TA', 'CA / CL', 'EQ / (TL + EQ)', 'quality_debt', 'Net Profit / Sales', 'ROA', 'ROE', 'bankruptcy'])


# Whatsmore, it was detected that the bankruptcy category was an 'object type', so it was converted into a 'categorical type'. In this way plotting the graphs would be easier and clarifier.

# In[4]:


d['bankruptcy'].replace({"b'0'": "0", "b'1'": "1"}, inplace=True)
d.bankruptcy = pd.to_numeric(d.bankruptcy)
d.bankruptcy = pd.Categorical(d.bankruptcy.replace({0: 'Not bankrupted', 1: 'Bankrupted'}))


# # Dealing with extremes values and NaN
# 
# It was detected that the data frame had very extremly outliers, which had no sense. A ROE = -1600 would imply that the firms had a loss about 160000% of his Equity. This is an extreme example but many features had the same issue. In order to eliminate this inconsistency it was decided to drop the 5% of extremes values (2.5% for each tail).

# In[5]:


filt_d = d.loc[:, d.columns != 'bankruptcy']

low = .025
high = .975
quant_d = filt_d.quantile([low, high])
print(quant_d)


# Here appeared some drastic measure. The data frame was pretty unbalanced and it was decided to fill those null valued with the mean of the variable. This changed significally the distribution of the values of each feature as can be seen in the pairplots. It was considered and then rejected to drop out all the observations (firms) that had some null value in some feature because the dataset would be reduced by more than 40%. 

# In[6]:


filt_d = filt_d.apply(lambda x: x[(x>quant_d.loc[low,x.name]) & (x < quant_d.loc[high,x.name])], axis=0)
filt_d = pd.concat([d.loc[:,'bankruptcy'], filt_d], axis=1)
filt_d = filt_d.fillna(filt_d.mean())
filt_d


# In[7]:


filt_d


# In[8]:


filt_d.describe().round(2)


#  # DATA VISUALIZATION 
#  
#  From now on can be seen how is the data distributed and how features interacts with each other. It is shown on the graph but it is worth reminding that blue dots corresponds to the bankrupted firms and the orange one to the survivals firms.

# In[9]:


plt.rcParams['figure.figsize'] = (10,8)

sns.pairplot(filt_d, hue="bankruptcy", height=2.5);


# In[10]:


d2 = filt_d[['Net Profit / Sales', 'ROA', 'ROE', 'bankruptcy']]


# Here are the financial indicators substracted in order to have a better understanding on how thos ratios are distributed and how they relationate.

# In[11]:


sns.pairplot(d2, hue="bankruptcy", diag_kind='hist');


# In[12]:


d3 = filt_d[['TL / TA', 'Working K / TA', 'CA / CL', 'EQ / (TL + EQ)', 'quality_debt', 'bankruptcy']]


# On the other hand, it was also considered that the laverage ratio's graphs should have a better look.

# In[13]:


sns.pairplot(d3, hue="bankruptcy", diag_kind='hist');


# # CORRELATION MATRIX
# 
# With a first sight anyone can realise that those features related to the laverage have a higher (in absolute values) correlation that with the financial ratios.
# 
# On the other hand it can also be deducted that thos variables related to the financial situation of the firm have a higher correlation with theyselves rather that with those that explains laverage behaivour.

# In[14]:


sns.heatmap(filt_d.corr().round(2), cmap=plt.cm.PuRd, vmin=-1, vmax=1, annot=True, linewidths=1);


# # GRAPHS
# 
# Since the data exploration is finished it is time to compare more precisely the differences that provoke a firm going bankrupted. On the following lines there can be observed how firms behaves depending on some variables and if they bankrupted.

# In[15]:


sns.boxenplot(data=filt_d, x='bankruptcy', y='quality_debt');


# In[16]:


sns.boxenplot(data=filt_d, x='bankruptcy', y='CA / CL');


# In[17]:


sns.boxenplot(data=filt_d, x='bankruptcy', y='EQ / (TL + EQ)');


# In[18]:


sns.violinplot(data=filt_d, x='bankruptcy', y='ROE', inner='quartiles');


# In[19]:


sns.boxenplot(data=filt_d, x='bankruptcy', y='ROA');


# ## Machine Learning

# In[20]:


from sklearn.model_selection import train_test_split


# In[21]:


dataset = filt_d


# In[22]:


cleanup = {"bankruptcy":     {"Not bankrupted": -1, "Bankrupted": 1}}


# In[23]:


dataset = dataset.replace(cleanup)


# In[24]:


label = 'bankruptcy'
features = [column for column in dataset.columns if column != label]
X = dataset[features]
y = dataset[label]


# In[25]:


X_tv, X_test, y_tv, y_test = train_test_split(X,y, shuffle=True, test_size=0.25, random_state=0)


# In[26]:


len(X), len(y), len(X_tv), len(y_tv), len(X_test), len(y_test)


# In[27]:


from sklearn.datasets import load_boston
from sklearn import tree
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ParameterGrid
from sklearn.svm import SVC
from sklearn.inspection import permutation_importance
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import multiprocessing


# In[28]:


train_test_split(X_tv, y_tv)


# # Decision Tree

# In[29]:


dtc = tree.DecisionTreeClassifier()
dtc = dtc.fit(X_tv, y_tv)
tree.plot_tree(dtc) 


# # SVC

# In[30]:


kfold=StratifiedKFold(n_splits=5, shuffle=True, random_state=0)


# In[31]:


from sklearn import svm
from sklearn.metrics import recall_score, accuracy_score, confusion_matrix
svc = svm.SVC()
svc.fit(X_tv, y_tv)
SVC()


# In[32]:


pred = svc.predict(X_test)
pred


# In[33]:


accuracy_score(y_test, pred)


# In[34]:


fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(confusion_matrix(y_test, pred), cmap=plt.cm.jet, square=True, annot=True, cbar=False, ax=ax)
ax.set_xlabel('Predicted class')
ax.set_ylabel('True class')
ax.set_xticklabels(['Bankrupt', 'Not Bankrupt'])
ax.set_yticklabels(['Bankrupt', 'Not Bankrupt']);


# In[35]:


len(pred[(pred == -1) & (y_test== 1)])


# # Gradient Boosting

# #### Optimal value of n_estimators

# In[37]:


train_scores = []
cv_scores    = []

estimator_range = range(1, 500, 25)

for n_estimators in estimator_range:
    
    model = GradientBoostingRegressor(
                n_estimators = n_estimators,
                loss         = 'ls',
                max_features = 'auto',
                random_state = 123
             )

    model.fit(X_tv, y_tv)
    predictions = model.predict(X = X_tv)
    rmse = mean_squared_error(
            y_true  = y_tv,
            y_pred  = predictions,
            squared = False
           )
    train_scores.append(rmse)
    
    scores = cross_val_score(
                estimator = model,
                X         = X_tv,
                y         = y_tv,
                scoring   = 'neg_root_mean_squared_error',
                cv        = 5,
                n_jobs    = multiprocessing.cpu_count() - 1,
             )
    
    cv_scores.append(-1*scores.mean())
    
fig, ax = plt.subplots(figsize=(6, 3.84))
ax.plot(estimator_range, train_scores, label="train scores")
ax.plot(estimator_range, cv_scores, label="cv scores")
ax.plot(estimator_range[np.argmin(cv_scores)], min(cv_scores),
        marker='o', color = "red", label="min score")
ax.set_ylabel("root_mean_squared_error")
ax.set_xlabel("n_estimators")
ax.set_title("Evolution cv-error vs number of trees")
plt.legend();
print(f"Optimal value of n_estimators: {estimator_range[np.argmin(cv_scores)]}")


# In[38]:


model = GradientBoostingRegressor(
            n_estimators = 26,
            loss         = 'ls',
            max_features = 'auto',
            random_state = 123
         )


# In[39]:


model.fit(X_tv, y_tv)


# In[40]:


model.get_params(deep=True)


# In[41]:


model.predict(X_tv)


# In[42]:


importance_predictors = pd.DataFrame(
                            {'predictor': dataset.drop(columns = "bankruptcy").columns,
                             'importance': model.feature_importances_}
                            )
print("Importance predictors in the model")
print("-------------------------------------------")
importance_predictors.sort_values('importance', ascending=False)


# In[43]:


importance = permutation_importance(
                estimator    = model,
                X            = X_tv,
                y            = y_tv,
                n_repeats    = 5,
                scoring      = 'neg_root_mean_squared_error',
                n_jobs       = multiprocessing.cpu_count() - 1,
                random_state = 123
             )


# In[44]:


df_importance = pd.DataFrame(
                    {k: importance[k] for k in ['importances_mean', 'importances_std']}
                 )
df_importance['feature'] = X_tv.columns
df_importance.sort_values('importances_mean', ascending=False)


# In[45]:


fig, ax = plt.subplots(figsize=(5, 6))

sorted_idx = importance.importances_mean.argsort()
ax.boxplot(
        importance.importances[sorted_idx].T,
        vert   = False,
        labels = dataset.drop(columns = "bankruptcy").columns[sorted_idx]
)
ax.set_title('Importance predictors (train)')
ax.set_xlabel('Increase error after permutation')
fig.tight_layout();


# In[46]:


predictions = model.predict(X = X_test)
rmse = mean_squared_error(
        y_true  = y_test,
        y_pred  = predictions,
        squared = False
       )
print(f"The error (rmse) of the test is: {rmse}")


# # Conclusions

# The intention of the study was to illustrate how machine learning can be exploited into predicting firms bankruptcy from financial indicators
# The models are seen to perform differently based on the hyper-parameters used, as well as the metric chosen for measuring performance.
# The dataset has a large number not going bankrup and very few bankrupt in all models. As a result, most models give high Accuracy, but perform poor when predicting bankruptcy. 
# We find that the ensemble model Gradient Boosting perform best on Training and Test datasets. They are also computationally fast, completing the training and scoring in a couple of minutes. 
